---
task_key: T-E04-F04-005
status: todo
feature: /home/jwwelbor/.claude/docs/plan/E04-task-mgmt-cli-core/E04-F04-epic-feature-queries
created: 2025-12-14
assigned_agent: general-purpose
dependencies: [T-E04-F04-004]
estimated_time: 4 hours
file_path: /home/jwwelbor/.claude/docs/tasks/todo/T-E04-F04-005.md
---

# Task: Integration Tests & Performance Validation

## Goal

Validate end-to-end functionality of all epic and feature query commands with realistic data, verify all acceptance criteria scenarios pass, and benchmark performance against PRD targets.

## Success Criteria

- [ ] Integration tests cover all 4 commands with realistic data
- [ ] All acceptance criteria scenarios from PRD pass
- [ ] Progress calculation accuracy verified (all test scenarios match expected percentages)
- [ ] Performance benchmarks meet PRD targets (epic list <100ms, epic get <200ms, etc.)
- [ ] Error handling tests verify correct exit codes and messages
- [ ] Empty result tests verify helpful messages
- [ ] Query performance validated with EXPLAIN QUERY PLAN
- [ ] No N+1 query problems detected
- [ ] Tests run successfully in parallel
- [ ] Test coverage >90% for all epic/feature query modules

## Implementation Guidance

### Overview

This phase validates the complete epic and feature query functionality through integration tests, performance benchmarks, and acceptance criteria verification. Tests must use realistic data volumes, verify mathematical accuracy of progress calculations, and ensure performance meets PRD targets.

### Key Requirements

- **Acceptance Criteria Scenarios**: See [PRD - Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F04-epic-feature-queries/prd.md#acceptance-criteria)
  - Epic listing with 5 epics displays all with correct progress
  - Epic E01 with 3 features (50%, 75%, 100%) shows 75% epic progress
  - Feature E01-F02 with 10 tasks (7 completed, 2 in_progress, 1 todo) shows 70% progress
  - Epic weighted average scenarios (see PRD #330-336)
  - All error scenarios (non-existent keys, invalid filters)

- **Performance Benchmarks**: See [PRD - Non-Functional Requirements - Performance](../docs/plan/E04-task-mgmt-cli-core/E04-F04-epic-feature-queries/prd.md#non-functional-requirements)
  - `pm epic list` <100ms for 100 epics
  - `pm epic get <key>` <200ms for epics with 50 features
  - `pm feature list` <100ms for 100 features
  - `pm feature get <key>` <200ms for features with 100 tasks
  - Progress calculations use efficient SQL (no N+1 queries)

- **Integration Test Coverage**:
  - All 4 commands (epic list, epic get, feature list, feature get)
  - All filters (--epic, --status, --sort-by)
  - JSON and table output modes
  - Error scenarios (non-existent keys, database errors, invalid filters)
  - Empty results (no epics, no features, filtered results empty)
  - Progress calculation edge cases

- **Query Performance Validation**:
  - Run EXPLAIN QUERY PLAN on all progress queries
  - Verify indexes are used (no full table scans for indexed queries)
  - Check for N+1 query patterns (log SQL statements during test)
  - Measure query execution time with realistic data volumes

### Files to Create/Modify

**New Files**:
- `tests/integration/test_epic_feature_queries.py` - Main integration test suite (~400 lines)
  - Test all acceptance criteria scenarios
  - Test all commands with realistic data
  - Test error handling and edge cases

- `tests/performance/test_query_performance.py` - Performance benchmarks (~200 lines)
  - Benchmark epic list with 100 epics
  - Benchmark epic get with 50 features
  - Benchmark feature get with 100 tasks
  - Verify no N+1 queries

- `tests/fixtures/epic_feature_data.py` - Test data fixtures (~100 lines)
  - Create realistic epic/feature/task hierarchies
  - Known progress values for verification

**Test Files**:
- Expand existing unit tests to ensure >90% coverage
- Add edge case tests for all identified scenarios

### Integration Points

- **E04-F01 Database**: Create test database with realistic data
  - Multiple epics with varying numbers of features
  - Features with varying numbers of tasks
  - Tasks with different statuses (completed, in_progress, todo, blocked)
  - Known progress values for verification

- **E04-F02 CLI Framework**: Use CliRunner for command testing
  - Test commands via Click's test runner
  - Capture stdout and exit codes
  - Test with different flag combinations

- **Progress Service** (T-E04-F04-001): Verify calculation accuracy
  - Compare calculated progress to expected values
  - Test all edge cases (zero tasks, zero features, weighted averages)

## Validation Gates

- **All Acceptance Criteria Pass**: See [PRD - Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F04-epic-feature-queries/prd.md#acceptance-criteria)
  - Epic Listing: All scenarios pass
  - Epic Details: All scenarios pass
  - Feature Listing: All scenarios pass
  - Feature Details: All scenarios pass
  - Progress Calculation: All scenarios pass
  - JSON Output: All scenarios pass
  - Error Handling: All scenarios pass
  - Table Formatting: All scenarios pass

- **Performance Benchmarks Meet Targets**:
  - `pm epic list` with 100 epics completes in <100ms
  - `pm epic get E01` with 50 features completes in <200ms
  - `pm feature get E01-F02` with 100 tasks completes in <200ms
  - Progress calculations use single queries (no N+1)
  - EXPLAIN QUERY PLAN shows index usage

- **Test Coverage**:
  - `pm/services/progress.py`: 100%
  - `pm/cli/epic.py`: >90%
  - `pm/cli/feature.py`: >90%
  - `pm/formatters/`: >85%
  - Overall: >90%

- **Error Handling Verification**:
  - Non-existent epic/feature returns exit code 1
  - Database errors return exit code 2
  - Invalid filters return exit code 1 with valid options
  - Error messages match PRD specifications

## Context & Resources

- **PRD**: [Complete Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F04-epic-feature-queries/prd.md#acceptance-criteria)
  - All acceptance criteria scenarios must pass
  - Performance requirements in Non-Functional Requirements section
  - Error message specifications in Functional Requirements section

- **Testing Patterns**: Reference E04-F01 integration tests
  - Database fixture setup
  - Transaction management in tests
  - Test data creation patterns

## Notes for Agent

- **Realistic Data Volumes**: Test with 100 epics, 50 features per epic, 100 tasks per feature for performance benchmarks. Use smaller datasets for functional tests.
- **Known Progress Values**: Create test fixtures with pre-calculated progress values. Verify calculations match expected values exactly.
- **EXPLAIN QUERY PLAN**: Use SQLite's EXPLAIN QUERY PLAN to verify query efficiency. Look for "USING INDEX" in output.
- **N+1 Detection**: Enable SQL logging during tests (`echo=True` in database config). Count queries. Should be constant regardless of data volume.
- **Performance Measurement**: Use `time.perf_counter()` for accurate timing. Run benchmarks multiple times and average. Exclude database setup time.
- **Test Isolation**: Each test should create its own database or use transactions. Tests must not interfere with each other.
- **Acceptance Criteria Coverage**: Create one test per acceptance criteria scenario. Use scenario description as test name for traceability.
- **Error Testing**: Mock database errors (connection failures) to test exit code 2 paths. Use invalid inputs to test exit code 1 paths.
