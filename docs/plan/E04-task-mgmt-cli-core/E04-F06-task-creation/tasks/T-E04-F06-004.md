---
task_key: T-E04-F06-004
status: todo
feature: /home/jwwelbor/.claude/docs/plan/E04-task-mgmt-cli-core/E04-F06-task-creation
created: 2025-12-14
assigned_agent: general-purpose
dependencies: ["T-E04-F06-003"]
estimated_time: 6 hours
file_path: /home/jwwelbor/.claude/docs/tasks/todo/T-E04-F06-004.md
---

# Task: Integration & Testing

## Goal

Implement comprehensive integration tests, performance benchmarks, and end-to-end validation to ensure task creation works reliably across all scenarios, meets performance targets, and integrates correctly with all dependencies.

## Success Criteria

- [ ] Integration tests validate complete task creation workflow
- [ ] Performance benchmarks verify task creation completes in <500ms
- [ ] Concurrent creation tests prevent duplicate keys
- [ ] All acceptance criteria from PRD verified with automated tests
- [ ] Error scenarios tested (validation failures, database errors, file errors)
- [ ] Template rendering tested for all 6 agent types
- [ ] Dependency chain tests verify cross-task references
- [ ] JSON output format validated with schema
- [ ] Human-readable output format verified
- [ ] Test coverage >80% for all task creation modules
- [ ] All PRD acceptance criteria have corresponding automated tests
- [ ] Documentation updated with usage examples and common patterns

## Implementation Guidance

### Overview

This task validates that all components work together correctly and that the task creation feature meets all functional and non-functional requirements from the PRD. Tests should cover happy paths, error paths, edge cases, and performance benchmarks.

### Key Requirements

- **Integration Test Coverage**: Per [PRD - Acceptance Criteria](../prd.md#acceptance-criteria)
  - Basic task creation (all required fields)
  - Task creation with optional fields (description, priority, dependencies)
  - Automatic key sequencing (001, 002, ..., 099, 100, ...)
  - Input validation for all fields
  - Dependency validation
  - Template application for each agent type
  - Frontmatter generation correctness
  - Atomic creation (rollback on failure)
  - JSON output format
  - Command output format

- **Performance Benchmarks**: Per [PRD - Non-Functional Requirements - Performance](../prd.md#non-functional-requirements)
  - Task creation (DB insert + file write) completes in <500ms
  - Key generation query completes in <50ms
  - Template rendering completes in <100ms
  - Concurrent creation doesn't create duplicate keys

- **Error Scenario Tests**: Per [PRD - Acceptance Criteria - Input Validation](../prd.md#input-validation)
  - Non-existent epic error
  - Non-existent feature error
  - Invalid agent type error
  - Invalid priority error
  - Non-existent dependency error
  - Missing required flag error
  - File already exists error
  - Database constraint violation error
  - File permission error

- **Template Tests**: Per [PRD - Acceptance Criteria - Template Application](../prd.md#template-application)
  - Frontend template includes component specs
  - Backend template includes API endpoints
  - API template includes request/response specs
  - Testing template includes test scenarios
  - DevOps template includes deployment configs
  - General template has flexible structure

- **Frontmatter Validation**: Per [PRD - Acceptance Criteria - Frontmatter Generation](../prd.md#frontmatter-generation)
  - All required fields present in frontmatter
  - Field values match input data
  - YAML structure is valid (parseable with gopkg.in/yaml.v3)
  - Arrays formatted correctly
  - Timestamps in ISO 8601 format

### Files to Create/Modify

**New Files**:
- `internal/taskcreation/integration_test.go` - End-to-end workflow tests (~400 lines)
- `internal/taskcreation/error_test.go` - Error path tests (~300 lines)
- `internal/templates/validation_test.go` - Template validation tests (~250 lines)
- `internal/taskcreation/benchmark_test.go` - Performance benchmarks (~200 lines)
- `internal/taskcreation/testdata/` - Test fixtures and sample data
- `docs/examples/task_creation_examples.md` - Usage examples and patterns (~200 lines)

**Modified Files**:
- `README.md` - Add task creation usage section
- `internal/taskcreation/testing.go` - Add shared test helpers

### Integration Points

- **Database (E04-F01)**: Tests verify database records created correctly
- **CLI Framework (E04-F02)**: Tests invoke CLI commands and verify output
- **Folder Management (E04-F05)**: Tests verify files created in correct locations
- **All Task Creation Components**: Tests verify template renderer, validators, key generator work together

### Test Categories

**Category 1: Happy Path Integration Tests**
- Test creating task with minimal fields (only required)
- Test creating task with all fields (including optional)
- Test creating task with single dependency
- Test creating task with multiple dependencies
- Test creating tasks in sequence (verify auto-incrementing keys)
- Test creating task for each agent type (6 templates)

**Category 2: Error Path Tests**
- Test validation errors (epic, feature, agent, priority, dependencies)
- Test missing required flags
- Test file already exists error
- Test database transaction rollback scenarios
- Test template rendering errors

**Category 3: Atomicity Tests**
- Test database success + file failure → no database record
- Test database failure → no file created
- Test partial transaction rollback
- Test concurrent creation safety

**Category 4: Output Format Tests**
- Test human-readable output format
- Test --json output format
- Test JSON schema validation
- Test output with different field combinations

**Category 5: Template Validation Tests**
- Test each template renders correctly
- Test frontmatter YAML validity
- Test optional field handling (description, dependencies)
- Test special characters in title/description

**Category 6: Performance Tests**
- Benchmark complete task creation (<500ms)
- Benchmark key generation query (<50ms)
- Benchmark template rendering (<100ms)
- Test creation with 100 existing tasks (verify performance doesn't degrade)

**Category 7: Integration with Existing Features**
- Test task appears in `shark task list` after creation (if E04-F03 implemented)
- Test task can be started with `shark task start` (if E04-F03 implemented)
- Test folder management integration (atomic operations)

### Test Data Fixtures

Create reusable test fixtures:
- Sample epic data (E01, E02 with different statuses)
- Sample feature data (F01, F02 belonging to different epics)
- Sample task data (existing tasks for testing dependencies)
- Invalid input data (for validation error tests)
- Edge case data (999th task, special characters, very long descriptions)

### Validation Checklist

Map each PRD acceptance criterion to a test:
- [ ] Basic Task Creation → `test_create_task_with_required_fields`
- [ ] Automatic Key Sequencing → `test_sequential_key_generation`
- [ ] Input Validation → `test_validation_errors`
- [ ] Dependency Validation → `test_dependency_validation`
- [ ] Template Application → `test_all_agent_templates`
- [ ] Frontmatter Generation → `test_frontmatter_validity`
- [ ] Atomic Creation → `test_transaction_rollback`
- [ ] JSON Output → `test_json_output_format`
- [ ] Optional Fields → `test_optional_field_handling`
- [ ] Command Output → `test_human_readable_output`

## Validation Gates

- **Test Coverage**: Per [PRD - Non-Functional Requirements - Maintainability](../prd.md#non-functional-requirements)
  - >80% code coverage for all task creation modules
  - Run: `go test -cover ./internal/taskcreation/... ./internal/templates/...`
  - Generate detailed report: `go test -coverprofile=coverage.out && go tool cover -html=coverage.out`
  - Verify all critical paths covered

- **Performance Benchmarks**: All benchmarks meet targets
  - Task creation: <500ms average over 10 runs
  - Key generation: <50ms average over 100 runs
  - Template rendering: <100ms average over 50 runs
  - Run with: `go test -bench=. -benchmem ./internal/taskcreation/...`

- **Acceptance Criteria Coverage**: All PRD acceptance criteria have tests
  - Basic Task Creation: ✓
  - Automatic Key Sequencing: ✓
  - Input Validation: ✓
  - Dependency Validation: ✓
  - Template Application: ✓
  - Frontmatter Generation: ✓
  - Atomic Creation: ✓
  - JSON Output: ✓
  - Optional Fields: ✓
  - Command Output: ✓

- **Integration Tests**: All integration tests pass
  - Run: `go test -v ./internal/taskcreation/... -tags=integration`
  - Verify no failures or errors
  - Use build tags to separate unit and integration tests

- **Error Handling**: All error scenarios tested and verified
  - Error messages are specific and actionable
  - Exit codes are correct (1 for errors)
  - Rollback behavior verified

- **Template Validation**: All templates produce valid output
  - YAML frontmatter parseable
  - Markdown structure correct
  - Agent-specific sections present
  - No rendering errors

## Context & Resources

- **PRD**: [Task Creation & Templating PRD](../prd.md)
  - [All Acceptance Criteria](../prd.md#acceptance-criteria)
  - [Non-Functional Requirements - Performance](../prd.md#non-functional-requirements)
  - [Non-Functional Requirements - Reliability](../prd.md#non-functional-requirements)
- **Epic**: [E04 Task Management CLI](../../epic.md)
- **Testing Best Practices**: Reference E04-F01 test structure for patterns

## Notes for Agent

- **Test Organization**: Group tests by category using subtests or separate test functions. Use descriptive test names: `TestTaskCreation_WithAllFields`, `TestTaskCreation_ValidationError_InvalidEpic`
- **Table-Driven Tests**: Use Go's table-driven test pattern for multiple scenarios. Define test cases in a slice and iterate
- **Test Fixtures**: Create test helpers in `testing.go`. Use `testdata/` directory for sample files. Use Go's `t.TempDir()` for temporary directories
- **Temporary Files**: Use `t.TempDir()` for automatic cleanup of test files. Each test gets isolated directory
- **Database Isolation**: Use in-memory SQLite database (`:memory:`) for tests. Create fresh database per test using test helpers
- **Performance Testing**: Use Go's built-in benchmarking with `func BenchmarkXxx(b *testing.B)`. Run with `go test -bench=.`
- **Benchmarking Best Practices**: Use `b.ResetTimer()` after setup. Use `b.ReportAllocs()` to track memory allocations
- **Mocking Strategy**: Define interfaces for dependencies (Repository, Renderer). Use real implementations for integration tests, mocks for unit tests. Consider using `github.com/stretchr/testify/mock` for mocking
- **Assertion Quality**: Use `testify/assert` or `testify/require` for better error messages. `assert.Equal(t, "T-E01-F02-001", result.Key)` gives clear failures
- **Test Data**: Put test data in `testdata/` directory. Use `//go:embed` to embed test files if needed
- **Coverage Reporting**: Run `go test -coverprofile=coverage.out` then `go tool cover -html=coverage.out` to view HTML report. Identify untested branches
- **Documentation**: Document common usage patterns with executable examples using `Example` functions in test files
- **Edge Cases**: Test boundary conditions using table-driven tests with edge case inputs
- **Concurrent Testing**: Use goroutines with sync.WaitGroup to test concurrent creation. Use `go test -race` to detect race conditions
- **YAML Validation**: Use `gopkg.in/yaml.v3` to parse and validate frontmatter. Unmarshal into struct to verify structure
- **JSON Schema**: Define Go structs for JSON output. Unmarshal and verify fields match expected types and values
- **Build Tags**: Use `//go:build integration` to separate integration tests. Run unit tests with `go test` and integration with `go test -tags=integration`
- **Test Helpers**: Create helper functions like `createTestDB(t *testing.T) *sql.DB` and `setupTestTask(t *testing.T, db *sql.DB) *Task` for DRY tests
