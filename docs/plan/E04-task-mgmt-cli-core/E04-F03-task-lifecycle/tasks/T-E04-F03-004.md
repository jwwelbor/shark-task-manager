---
task_key: T-E04-F03-004
status: todo
feature: /home/jwwelbor/.claude/docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle
created: 2025-12-14
assigned_agent: general-purpose
dependencies: ["T-E04-F03-001", "T-E04-F03-002", "T-E04-F03-003"]
estimated_time: 8 hours
file_path: /home/jwwelbor/.claude/docs/tasks/todo/T-E04-F03-004.md
priority: 2
---

# PRP: Integration Testing & Workflow Validation

## Goal

Implement comprehensive integration tests that validate end-to-end task lifecycle workflows, verify atomic transactions between database and file operations, test concurrent command execution, and ensure all acceptance criteria from the PRD are met. This phase validates that all task lifecycle commands work together correctly in realistic scenarios.

## Success Criteria

- [ ] End-to-end workflow tests covering complete task lifecycle (todo → in_progress → ready_for_review → completed)
- [ ] Exception workflow tests (blocking, unblocking, reopening)
- [ ] Concurrent command execution tests (multiple agents working simultaneously)
- [ ] Transaction rollback tests (database/file operation failures)
- [ ] Dependency validation tests (next command excludes tasks with incomplete dependencies)
- [ ] History audit trail tests (verify every state change recorded)
- [ ] Feature progress calculation tests (progress updates correctly after state changes)
- [ ] Performance benchmarks met (list <100ms, next <50ms, transitions <200ms)
- [ ] All PRD acceptance criteria validated with integration tests
- [ ] Agent workflow simulation tests (JSON output parsing, error handling)
- [ ] Cross-platform compatibility verified (Linux/macOS/Windows if applicable)
- [ ] 100% pass rate on all integration tests
- [ ] Test coverage report shows >95% overall code coverage

## Implementation Guidance

### Overview

This phase creates integration tests that validate the entire task lifecycle system works correctly under realistic conditions. Unlike unit tests that verify individual components, integration tests ensure commands interact properly with the database, file system, and each other. You'll create test scenarios that simulate real agent and developer workflows, verify transaction atomicity, and validate performance under load.

### Key Requirements

- **End-to-End Workflow Tests**: Verify complete task lifecycle flows
  - Happy path: Create task → start → complete → approve (verify each state transition)
  - Agent workflow: next → start → complete (simulate agent automation)
  - Developer workflow: list → get → approve/reopen (simulate human review)
  - Verify database state, file locations, history records at each step
  - Confirm progress calculation updates correctly

- **Exception Workflow Tests**: Verify blocking and rework scenarios
  - Block workflow: start → encounter blocker → block (with reason)
  - Unblock workflow: block → resolve blocker → unblock → start again
  - Reopen workflow: complete → review → reopen → fix → complete → approve
  - Verify blocked tasks excluded from next command
  - Confirm file locations correct for blocked tasks (stay in place)

- **Transaction Atomicity Tests**: Verify rollback behavior from [PRD - Atomic Transactions](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#atomic-transactions)
  - Simulate file operation failure during state transition
  - Verify database transaction rolled back (status unchanged)
  - Confirm no history record created on rollback
  - Test with all state transition commands
  - Verify appropriate error messages and exit codes

- **Dependency Validation Tests**: Verify next command dependency checking from [PRD - Task Discovery](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#task-discovery-pm-task-next)
  - Create task with dependencies in various states
  - Verify next excludes tasks with incomplete dependencies
  - Test dependency in "todo" state (should exclude parent)
  - Test dependency in "blocked" state (should exclude parent)
  - Test dependency in "completed" state (should include parent)
  - Verify correct task selected when multiple candidates exist

- **Concurrent Execution Tests**: Verify system handles concurrent operations
  - Multiple agents calling next simultaneously (should get different tasks)
  - Concurrent state transitions on different tasks
  - Database locking prevents race conditions
  - Verify no duplicate task assignments
  - Test with threading/multiprocessing

- **History Audit Tests**: Verify complete history tracking from [PRD - History Recording](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#history-recording)
  - Execute complete workflow and retrieve history
  - Verify every state change has exactly one history record
  - Confirm history includes: old_status, new_status, agent, timestamp, notes
  - Verify history is immutable (no updates/deletes)
  - Confirm timestamps in correct chronological order

- **Performance Benchmarks**: Verify targets from [PRD - Performance](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#non-functional-requirements)
  - List 1,000 tasks completes in <100ms
  - Next command completes in <50ms
  - State transitions complete in <200ms (including file operations)
  - Use pytest-benchmark for consistent measurements
  - Test with realistic database sizes

- **Agent Workflow Simulation**: Verify agent use cases from [PRD - User Stories](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#user-stories)
  - Agent calls: next --agent=backend --json
  - Parse JSON output programmatically
  - Extract task key from JSON
  - Call: start <task-key>
  - Call: complete <task-key> --notes="..."
  - Verify entire workflow succeeds without human intervention
  - Test error handling (invalid task key, wrong state, etc.)

- **Acceptance Criteria Validation**: Map tests to [PRD - Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#acceptance-criteria)
  - Create test for each Given/When/Then scenario in PRD
  - Use descriptive test names matching acceptance criteria
  - Verify exact expected behavior from PRD
  - Document any deviations or additional validations

### Files to Create/Modify

**New Files**:
- `tests/integration/test_task_lifecycle_workflows.py` - End-to-end workflow tests (~400 lines)
- `tests/integration/test_task_lifecycle_transactions.py` - Transaction atomicity tests (~200 lines)
- `tests/integration/test_task_lifecycle_concurrency.py` - Concurrent execution tests (~200 lines)
- `tests/integration/test_task_lifecycle_performance.py` - Performance benchmark tests (~150 lines)
- `tests/integration/test_agent_workflow_simulation.py` - Agent automation tests (~200 lines)
- `tests/integration/fixtures/task_lifecycle.py` - Shared test fixtures (~150 lines)

**Modified Files**:
- `tests/conftest.py` - Add integration test fixtures and configuration
- `pyproject.toml` or `setup.py` - Add pytest-benchmark dependency

### Integration Points

- **All E04-F03 Tasks**: This task integrates all previous task lifecycle components
- **E04-F01 Database**: Uses real database with test fixtures
- **E04-F02 CLI Framework**: Executes commands via Click's testing utilities
- **E04-F05 Folder Management**: Verifies file operations complete correctly (or mocked)

## Validation Gates

- **Workflow Coverage**: All PRD workflows tested
  - Standard workflow (todo → in_progress → ready_for_review → completed)
  - Block workflow (todo/in_progress → blocked → todo)
  - Reopen workflow (ready_for_review → in_progress)
  - Query workflows (list with filters, get, next with dependencies)

- **Acceptance Criteria Coverage**: Every Given/When/Then from [PRD - Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#acceptance-criteria) has test
  - Task listing acceptance criteria (8 scenarios)
  - Task discovery acceptance criteria (6 scenarios)
  - Task start acceptance criteria (4 scenarios)
  - Task complete acceptance criteria (4 scenarios)
  - Task approve acceptance criteria (2 scenarios)
  - Task block acceptance criteria (4 scenarios)
  - Task unblock acceptance criteria (2 scenarios)
  - Task reopen acceptance criteria (2 scenarios)
  - State transition validation (2 scenarios)
  - History recording (2 scenarios)
  - Atomic transactions (2 scenarios)

- **Performance Benchmarks**: All targets met
  - `shark task list` (1,000 tasks) < 100ms
  - `shark task next` < 50ms
  - State transitions < 200ms
  - Document actual performance in test output

- **Concurrent Safety**: No race conditions detected
  - Multiple agents can work simultaneously
  - No duplicate task assignments
  - Database maintains consistency

- **Coverage Report**: Overall code coverage >95%
  - Run `pytest --cov=shark --cov-report=html --cov-report=term`
  - Generate HTML coverage report for review
  - Identify any uncovered edge cases

- **All Tests Pass**: 100% pass rate
  - Run `pytest tests/integration/ -v`
  - No flaky tests (run multiple times to verify)
  - All assertions validate expected behavior

## Context & Resources

- **PRD**: [Task Lifecycle Operations PRD](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md)
  - [All Acceptance Criteria](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#acceptance-criteria)
  - [User Stories](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#user-stories)
  - [Performance Requirements](../docs/plan/E04-task-mgmt-cli-core/E04-F03-task-lifecycle/prd.md#non-functional-requirements)
- **Testing Best Practices**: Use Click's CliRunner for command testing
- **Previous Tasks**: T-E04-F03-001, T-E04-F03-002, T-E04-F03-003 provide components to test

## Notes for Agent

- **Click CliRunner**: Use Click's testing utility to invoke commands: `runner = CliRunner(); result = runner.invoke(cli, ['task', 'list', '--json'])`
- **Database Fixtures**: Create test database with known data (epics, features, tasks with various states)
- **File Operation Mocking**: If E04-F05 not implemented, mock file operations to simulate success/failure scenarios
- **JSON Parsing**: Use `json.loads(result.output)` to parse JSON output and verify structure
- **Transaction Testing**: Simulate failures by mocking file operations to raise exceptions, verify database rollback
- **Performance Testing**: Use pytest-benchmark: `@pytest.mark.benchmark` or `benchmark(lambda: some_function())`
- **Concurrent Testing**: Use threading.Thread or multiprocessing to simulate concurrent agents
- **Test Organization**: Group related tests in classes (TestTaskListFiltering, TestTaskStateTransitions, etc.)
- **Descriptive Names**: Name tests after acceptance criteria: `test_task_list_filters_by_status_and_epic`
- **Fixture Reuse**: Create shared fixtures for common test data (sample tasks, epics, features)
- **Cleanup**: Use pytest fixtures with `yield` for setup/teardown to ensure test isolation
- **Coverage Gaps**: If coverage <95%, identify missing branches and add tests for edge cases
